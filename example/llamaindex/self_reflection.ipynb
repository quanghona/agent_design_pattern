{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "fed8f988",
            "metadata": {},
            "source": [
                "# Self-reflection pattern\n",
                "\n",
                "Self-reflection pattern contains at least 2 promtps: the prompt that perform task itself and prompt to reflect the previous response. In self-reflection pattern, 2 prompt are performed by same LLM model in separated fashion.\n",
                "\n",
                "In this example, we are using llama index API and an ollama model hosted locally.\n",
                "\n",
                "## Setup\n",
                "host an ollama instance by using either running a docker or a run a service following instruction from ollama's guide."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "60456f7b",
            "metadata": {},
            "outputs": [],
            "source": [
                "from a2a.types import AgentCapabilities, AgentCard, AgentSkill\n",
                "from aap_core.agent import AgentMessage\n",
                "from aap_core.orchestration import ReflectionAgent\n",
                "from aap_llamaindex.chain import ChatCausalMultiTurnsChain\n",
                "from llama_index.llms.ollama import Ollama"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "0aa5f382",
            "metadata": {},
            "outputs": [],
            "source": [
                "model = Ollama(model=\"qwen3:4b-thinking-2507-q4_K_M\", base_url=\"192.168.55.1:11434\", request_timeout=400, context_window=32768)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0f1ac89a",
            "metadata": {},
            "outputs": [],
            "source": [
                "system_prompt_task = \"\"\"/no_think You are a helpful coding assistant.\n",
                "You task is to write a python function and return the implementation of the function.\n",
                "Some requirements:\n",
                "- The logic is clear and easy to understand.\n",
                "- The function arguments and return values (if any) should be typed.\n",
                "- If the function is too long (for example greater than 80 lines), split the logic into multiple smaller functions.\n",
                "- All functions should have docstring explanation. In the explanation, there should be an simple example to illustrate the function and how to call it.\n",
                "- The response should contain function with docstring explanation. And DO NOT contain explanation outside of the code\n",
                "\"\"\"\n",
                "user_prompt_task = \"{query}\"\n",
                "task_chain = ChatCausalMultiTurnsChain(\n",
                "    model=model,\n",
                "    system_prompt=system_prompt_task,\n",
                "    user_prompt_template=user_prompt_task,\n",
                "    base_url=\"192.168.55.1:11434\",\n",
                "    thinking=False,\n",
                "    request_timeout=400, context_window=32768)\n",
                "task_chain.final_response_as_context(\"response\")\n",
                "\n",
                "system_prompt_reflection = \"\"\"/no_think You are a excellent code reviewer and refactor.\n",
                "Given a function implementation and it explanation, your task is to review and code and correct if contains any mistake.\n",
                "Some note:\n",
                "- For the implementation, check if the orignal query and suggested implementation are match.\n",
                "- Is there any syntax error in the code.\n",
                "- For the explanation, verify if the docstring follows Google style docstring.\n",
                "- In the docstring, make sure to have an example to call the function.\n",
                "\n",
                "Make sure the final output only contain full function code, inline code comment and docstring, nothing else.\"\"\"\n",
                "user_prompt_reflection = \"Input query: {query}\\n\\nFunction implementation: {context_response}\"\n",
                "# Use the same llm for task and self-reflection\n",
                "reflection_chain = ChatCausalMultiTurnsChain(\n",
                "    model=model,\n",
                "    system_prompt=system_prompt_reflection,\n",
                "    user_prompt_template=user_prompt_reflection,\n",
                "    base_url=\"192.168.55.1:11434\",\n",
                "    thinking=False,\n",
                "    request_timeout=400, context_window=32768)  # use the same model for self-reflection\n",
                "\n",
                "def state_callback(state: str):\n",
                "    print(f\"agent state: {state}\")\n",
                "reflection_skill = AgentSkill(\n",
                "    id='reflection-skill',\n",
                "    name=\"reflection skill\",\n",
                "    description=\"self-reflection skill\",\n",
                "    tags=['reflection']\n",
                ")\n",
                "reflection_card = AgentCard(\n",
                "    name=\"reflection agent\",\n",
                "    description=\"self-reflection agent\",\n",
                "    skills=[reflection_skill],\n",
                "    capabilities=AgentCapabilities(),\n",
                "    default_input_modes=['text'],\n",
                "    default_output_modes=['text'],\n",
                "    url=\"localhost\",\n",
                "    version=\"0.1.0\"\n",
                ")\n",
                "reflection_agent = ReflectionAgent(card=reflection_card, chain_task=task_chain, chain_reflection=reflection_chain, state_change_callback=state_callback)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "d504019e",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "agent state: reflection agent:running\n",
                        "agent state: reflection agent:reflecting\n",
                        "agent state: reflection agent:idle\n",
                        "reflection agent\n",
                        "\n",
                        "\n",
                        "def min_pal_partition_cuts(s: str) -> int:\n",
                        "    \"\"\"Return the minimum number of cuts needed to partition the string into palindromic substrings.\n",
                        "\n",
                        "    Example: For s = \"aab\", the minimum cuts is 1 (partition [\"aa\", \"b\"]).\n",
                        "\n",
                        "    Args:\n",
                        "        s (str): Input string\n",
                        "\n",
                        "    Returns:\n",
                        "        int: Minimum cuts needed\n",
                        "\n",
                        "    Time Complexity: O(n^2)\n",
                        "    Space Complexity: O(n^2)\n",
                        "    \"\"\"\n",
                        "    n = len(s)\n",
                        "    is_pal = [[False] * n for _ in range(n)]\n",
                        "\n",
                        "    for i in range(n):\n",
                        "        is_pal[i][i] = True\n",
                        "\n",
                        "    for i in range(n - 1):\n",
                        "        is_pal[i][i + 1] = (s[i] == s[i + 1])\n",
                        "\n",
                        "    for length in range(3, n + 1):\n",
                        "        for i in range(n - length + 1):\n",
                        "            j = i + length - 1\n",
                        "            if s[i] == s[j] and is_pal[i + 1][j - 1]:\n",
                        "                is_pal[i][j] = True\n",
                        "\n",
                        "    dp = [float('inf')] * (n + 1)\n",
                        "    dp[0] = -1\n",
                        "\n",
                        "    for i in range(1, n + 1):\n",
                        "        for j in range(i):\n",
                        "            if is_pal[j][i - 1]:\n",
                        "                dp[i] = min(dp[i], dp[j] + 1)\n",
                        "\n",
                        "    return dp[n]\n",
                        "reflection agent\n",
                        "\n",
                        "\n",
                        "def min_pal_partition_cuts(s: str) -> int:\n",
                        "    \"\"\"Return the minimum number of cuts needed to partition the string into palindromic substrings.\n",
                        "\n",
                        "    Example:\n",
                        "        >>> min_pal_partition_cuts(\"aab\")\n",
                        "        1\n",
                        "\n",
                        "    Args:\n",
                        "        s (str): Input string\n",
                        "\n",
                        "    Returns:\n",
                        "        int: Minimum cuts needed\n",
                        "\n",
                        "    Time Complexity: O(n^2)\n",
                        "    Space Complexity: O(n^2)\n",
                        "    \"\"\"\n",
                        "    n = len(s)\n",
                        "    is_pal = [[False] * n for _ in range(n)]\n",
                        "\n",
                        "    for i in range(n):\n",
                        "        is_pal[i][i] = True\n",
                        "\n",
                        "    for i in range(n - 1):\n",
                        "        is_pal[i][i + 1] = (s[i] == s[i + 1])\n",
                        "\n",
                        "    for length in range(3, n + 1):\n",
                        "        for i in range(n - length + 1):\n",
                        "            j = i + length - 1\n",
                        "            if s[i] == s[j] and is_pal[i + 1][j - 1]:\n",
                        "                is_pal[i][j] = True\n",
                        "\n",
                        "    dp = [float('inf')] * (n + 1)\n",
                        "    dp[0] = -1\n",
                        "\n",
                        "    for i in range(1, n + 1):\n",
                        "        for j in range(i):\n",
                        "            if is_pal[j][i - 1]:\n",
                        "                dp[i] = min(dp[i], dp[j] + 1)\n",
                        "\n",
                        "    return dp[n]\n"
                    ]
                }
            ],
            "source": [
                "# Take a leetcode as an example. Source: https://leetcode.com/problems/palindrome-partitioning-ii/description/\n",
                "query = \"\"\"Write python function(s) to solve the following problem:\n",
                "Given a string s, partition s such that every substring of the partition is a palindrome.\n",
                "Return the minimum cuts needed for a palindrome partitioning of s.\n",
                "\n",
                "Example 1:\n",
                "Input: s = \"aab\"\n",
                "Output: 1\n",
                "Explanation: The palindrome partitioning [\"aa\",\"b\"] could be produced using 1 cut.\n",
                "\n",
                "Example 2:\n",
                "Input: s = \"a\"\n",
                "Output: 0\n",
                "\n",
                "Example 3:\n",
                "Input: s = \"ab\"\n",
                "Output: 1\n",
                "\n",
                "Constraints:\n",
                "1 <= s.length <= 2000\n",
                "s consists of lowercase English letters only.\"\"\"\n",
                "\n",
                "final_message = reflection_agent.execute(AgentMessage(query=query))\n",
                "for response in final_message.responses:\n",
                "    name, msg = response\n",
                "    print(name)\n",
                "    print(msg)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "llamaindex",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
